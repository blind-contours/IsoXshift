---
title: "Isobolic Interaction Identification and Estimation using Data-Adaptive Stochastic Interventions"
author: "David McCoy"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: ../inst/references.bib
vignette: >
  %\VignetteIndexEntry{Isobolic Interaction Identification and Estimation using Data-Adaptive Stochastic Interventions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r libraries, warning=FALSE}
library(IsoXshift)
library(devtools)
library(dplyr)
library(kableExtra)
library(sl3)

seed <- 429153
set.seed(seed)
```

## Motivation 

The motivation behind the package IsoXshift is to simulate isobolic interaction definitions used in toxicology. In areas of climate change or pollutant control, we are interested in identifying the most efficient set of interventions that lead to a target outcome level and we also want to know what the expected outcome would be if we tried to implement such a targeted intervention on a population understanding that it's not realistically to give all individuals the exact exposure levels due to different likelihoods of exposure. This package identifies the exposure levels for two exposures that, if set, result in the target outcome with minimum change from the original exposure distribution. Then it estimates the expected outcome if we were to try and get the population as close as possible to this optimal intervention strategy. This package analysis mixed exposures, finding the exposure relationship with the strongest synergist or antagonistic relationship and estimates a policy that tries to change people's exposures to these target levels.  


## Target Parameter 

IsoXshift uses data-adaptive machine learning methods to identify the exposures with the most synergy. This is defined based on our target parameter: 

$$

\begin{align}
  &\Psi(A_i, A_j, W) = \underset{c_i, c_j}{\mathrm{argmin}} 
  \left(\frac{|c_i - \mu_{A_i}| + |c_j - \mu_{A_j}|}{2}\right) \\
  &\text{subject to the following constraints:} \nonumber \\
  &\left|\mathbb{E}_{W, \textbf{A}_{-i,j}} [\mathbb{E} \left[Y \mid A_i = c_i, A_j = c_j, \textbf{A}_{-i,j}, W \right]] - Y_{\text{target}}\right| \leq \epsilon \\
  &c_i \in \text{supp}(A_i), \ c_j \in \text{supp}(A_j) \nonumber
\end{align}

$$

This target findings the minimal shift that results in an expectation that is the same (or close to) a target outcome. This finds the most efficient intervention (most synergistic or antagonistic relationship) that results in a target outcome.

## G-computation Approximation

To identify the most efficient exposure set which gets to the target outcome we employ a g-computation framework. We first construct a super learner, or ensemble of machine learning algorithms. We then construct all two-way combinations and for each combination, on a grid between min and max of the exposures, we get predictions through the model if the exposures are set to the various levels. At each iteration we calculate the mean difference between the new intervention level and the mean natural exposure level. We collect all expectations that are close to our target outcome and then select the ones with this lowest change from the natural exposure levels. This gives us the exposure relationship and levels of intervention, that result in the target outcome.  


## Sample Splitting 

Because we don't know which exposure relationship and intervention to estimate our synergy interaction parameter on a priori, we need to split our data and in the training fold find the exposure sets, using the above g-computation framework, and in an estimation sample, efficiently estimate the interaction parameter for these exposure sets so that we can get valid confidence intervals for these data-adaptively identified interactions. This is done using targeted learning.


## Targeted Learning

We use targeted minimum loss based estimation (TMLE) to debias our initial outcome estimates given a shift in exposure such that the resulting estimator is asymptotically unbiased and has the smallest variance. This procedure involves constructing a least favorable submodel which uses a ratio of conditional exposure densities under shift and now shift as a covariate to debias the initial estimates. More details of targeted learning for stochastic shifts are here @diaz2012population


## Data Adaptive Delta

Setting two exposures to specific levels, say PFAS 1 = 0.1, and PFAS = 0.3, is not a pathwise differentiable parameter. Furthermore, it is also not possible realistically to set all individual's exposure levels to one specific dose. We cannot, in the real world, for everyone to have PFAS exposure of 0.1. What we can do is say that PFAS 1 = 0.1, and PFAS = 0.3 has been found to most effectively reduce an outcome, like diabetes, to levels that are deemed healthy. We want to simulate the impact of an intention to intervene policy where we were to shift people's exposure levels towards this reduced amount, enforced by some policy, but respecting that people will not get to that amount exactly. People are restricted to how close they can get to this oracle level based on their likelihood of exposure. In the package `hn_trunc_thresh` indicates how far we can shift someone towards the new policy levels before we do not have support in our data to move them further, meaning their likelihood of the new intervention level is too far their observed exposure likelihood. For example if hn_trunc_thresh is set to 10, this means we are only willing a 10 fold change to their natural likelihood. This restriction ensures we are not creating estimates that are unfeasible in the real world. 

## Inputs and Outputs 

IsoXshift takes in covariates, exposures, an outcome, target_outcome_lvl, which is the level of the outcome we want to reach, epsilon, which is the amount of room around the target we allow for, the estimator (tmle or one step) number of folds for the CV procedure, and parallelization parameters. 

The package then outputs K-fold specific results, the result found in each fold, and the oracle result which is the parameter that is pooled across all the folds. For example, we will get the fold specific synergy results for exposure sets found to have the most synergy to our target outcome. The pooled parameter is the pooling these estimates across the folds.


If the same exposure sets are used across the folds, the pooled estimate is interpretable and has greater power compared to k-fold. If not, the analyst must investigate the k-fold specific results and report consistency of findings. 

## NHANES Data

Here we will try and replicate the analysis by Gibson et. al., 
https://ehjournal.biomedcentral.com/articles/10.1186/s12940-019-0515-1

and Mitro et. al, 
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4858394/

Who used the NHANES 2001-2002 data to investigate 18 POP exposures on telomere length. 

Because reduced telomere length is associated with more biological aging and higher morbidity, we 
will try and identify which region in the POP space would maximize telomere length. That is, 
we want to find the region that, if we created a regulation on those chemicals, would result in maximal change in elongating telomere length. 

Let's first load the data and investigate the variables: 

```{r NHANES data and variables}
data("NHANES_eurocim")

exposures <- c("LBX074LA", # PCB74 Lipid Adj (ng/g)
               "LBX099LA", # PCB99 Lipid Adj (ng/g)
               "LBX118LA", # PCB118 Lipid Adj (ng/g)
               "LBX138LA", # PCB138 Lipid Adj (ng/g)
               "LBX153LA", # PCB153 Lipid Adj (ng/g)
               "LBX170LA", # PCB170 Lipid Adj (ng/g)
               "LBX180LA", # PCB180 Lipid Adj (ng/g)
               "LBX187LA", # PCB187 Lipid Adj (ng/g)
               "LBX194LA", # PCB194 Lipid Adj (ng/g)
               "LBXD03LA", # 1,2,3,6,7,8-hxcdd Lipid Adj (pg/g)
               "LBXD05LA", # 1,2,3,4,6,7,8-hpcdd Lipid Adj (pg/g)
               "LBXD07LA", # 1,2,3,4,6,7,8,9-ocdd Lipid Adj (pg/g)
               "LBXF03LA", # 2,3,4,7,8-pncdf Lipid Adj (pg/g)
               "LBXF04LA", # 1,2,3,4,7,8-hxcdf Lipid Adj (pg/g)
               "LBXF05LA", # 1,2,3,6,7,8-hxcdf Lipid Adj (pg/g)
               "LBXF08LA", # 1,2,3,4,6,7,8-hxcdf Lipid Adj (pg/g)
               "LBXHXCLA", # 3,3',4,4',5,5'-hxcb Lipid Adj (pg/g)
               "LBXPCBLA") # 3,3',4,4',5-pcnb Lipid Adj (pg/g)

NHANES_eurocim <- NHANES_eurocim[complete.cases(NHANES_eurocim[, exposures]), ]

outcome <- "TELOMEAN"

covariates <- c("LBXWBCSI", # White blood cell count (SI)
                "LBXLYPCT", # Lymphocyte percent (%)
                "LBXMOPCT", # Monocyte percent (%)
                "LBXEOPCT", # Eosinophils percent (%)
                "LBXBAPCT", # Basophils percent (%)
                "LBXNEPCT", # Segmented neutrophils percent (%)
                "male", # Sex
                "age_cent", # Age at Screening, centered
                "race_cat", # race
                "bmi_cat3", # Body Mass Index (kg/m**2)
                "ln_lbxcot", # Cotinine (ng/mL), log-transformed
                "edu_cat") # Education Level - Adults 20+


```

To improve consistency in the region we find to be the maximizing region, it is best to remove an exposure of highly correlated sets, we do that here: 

```{r remove correlated exposures}
# Calculate the correlation matrix for the exposures
cor_matrix <- cor(NHANES_eurocim[, exposures], use = "complete.obs")

# Set a threshold for high correlation
threshold <- 0.8

# Find pairs of highly correlated exposures
highly_correlated_pairs <- which(abs(cor_matrix) > threshold & lower.tri(cor_matrix), arr.ind = TRUE)

# Initiate a vector to keep track of exposures to remove
exposures_to_remove <- c()

# Loop through the highly correlated pairs and decide which exposure to remove
for (pair in seq_len(nrow(highly_correlated_pairs))) {
  row <- highly_correlated_pairs[pair, "row"]
  col <- highly_correlated_pairs[pair, "col"]

  if (!(colnames(cor_matrix)[row] %in% exposures_to_remove)) {
    exposures_to_remove <- c(exposures_to_remove, colnames(cor_matrix)[row])
  }
}

# Keep only uncorrelated exposures
exposures_to_keep <- setdiff(exposures, exposures_to_remove)

```


```{r impute mean for covariates}

# Assuming NHANES_eurocim is your dataframe:
NHANES_eurocim_imputed <- NHANES_eurocim %>%
  mutate(across(all_of(covariates), ~ifelse(is.na(.), mean(., na.rm = TRUE), .)))

```


## Run IsoXshift 

```{r run IsoXshift for NHANES, warning=FALSE, message=FALSE}

ptm <- proc.time()

w <- NHANES_eurocim_imputed[, covariates]
a <- NHANES_eurocim_imputed[, exposures_to_keep]
y <- NHANES_eurocim_imputed$TELOMEAN

n_folds <- 5 

NIEHS_results <- IsoXshift(
  w = w,
  a = a,
  y = y,
  n_folds = n_folds,
  num_cores = 6,
  outcome_type = "continuous",
  seed = seed,
  target_outcome_lvl = mean(y) + mean(y) * .1,
  epsilon = 0.2
)

proc.time() - ptm
```


## K Fold Results

Let's first look at the results for each fold: 

```{r k fold results, eval = TRUE}
k_fold_results <- do.call(rbind, NIEHS_results$`K-fold Results`)
rownames(k_fold_results) <- NULL

k_fold_results %>%
  kableExtra::kbl(caption = "K Fold NHANES IsoXshift Results") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

Here, we find that the synergistic interaction between LBXF03LA-LBXF04LA most efficiently gets to our target outcome of 10 \% longer telomere length. This means that these two exposure need to changed the least together to get to our target outcome and therefore represent the most efficient intervention policy. 

We can look at what the target interventions are for each fold, this represents our oracle point parameter or the exposure levels for these two exposures that result in the 10\% increase while being as close to the observed distribution as possible.


## Oracle Intervention Levels

```{r oracle point results}

oracle_targets <- NIEHS_results$`K Fold Oracle Targets`
oracle_targets <- do.call(rbind, oracle_targets)
oracle_targets$Fold <- seq(1:n_folds)

oracle_targets %>%
  kableExtra::kbl(caption = "Oracle Intervention Level Targets by Fold") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

Above, this table shows the most synergistic relationship intervention targets per fold. Observed levels are the mean for the training data for the respective exposure in the respective fold. Intervened level is the level the exposure was set to in tandem with the other exposure to meet the target outcome. Avg Diff is the average difference between the intervened levels and natural exposure levels. Difference is the deviation from the target outcome level and should be within epsilon. 

## Pooled Target Parameter 

```{r pooled results}

pooled_target_param <- NIEHS_results$`Oracle Pooled Results`

pooled_target_param %>%
  kableExtra::kbl(caption = "Pooled Shift Towards Target") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

These results show the pooled shift results across the folds. Going back to our target interention levels, if we average the Intervened Levels and Observed Levels, our average intervened level is 6.489474 for LBXF03LA compared to observed 6.703457. For LBXF04LA the average intervened level was 5.828421 compared to the average observed exposure of 6.44141. This means we are trying to slightly reduce both exposures in order to increase telomere length by 10\%. We are trying to get the population as close to these two exposure levels without violating positivity, meaning here that the likelihood of the new exposure levels is no more than 10 times different than the observed exposure likelihood (this level is default in the IsoXshift run).

We can see that, although the results are consistent for the oracle point parameter, estimation is a different story, we don't see a positive or significant result for our shifts, which indicates we can't move the population close enough to the oracle intervention level. 


