% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Analysis of Variance using Super Learner with Data-Adaptive Stochastic Interventions},
  pdfauthor={David McCoy},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Analysis of Variance using Super Learner with Data-Adaptive
Stochastic Interventions}
\author{David McCoy}
\date{12/5/2021}

\begin{document}
\maketitle

\hypertarget{motivation}{%
\subsection{Motivation}\label{motivation}}

In many environmental epidemiology studies the analyst is interested in
the joint impact of a mixed exposure. That is, rather than a single
exposure \(A\), \(A\) is a vector of exposures of which the most
important individual variables and variable sets are unknown. For
example, in the analysis of a mixture of metals or air pollutants it is
not known a priori what variable(s) contribute to an outcome \(Y\) of
interest after controlling for baseline covariates \(W\) and/or what
interactions/effect modifications these exposures may have. Therefore,
it is necessary to identify:

\begin{itemize}
\tightlist
\item
  Individual variables in a mixture that have explanatory power on an
  outcome
\item
  A baseline covariate that modifies the impact of a mixture variable or
  set of mixture variables
\item
  Sets of mixture variables that have synergistic relationships.
\end{itemize}

\hypertarget{data-adaptive-machine-learning}{%
\subsubsection{Data-Adaptive Machine
Learning}\label{data-adaptive-machine-learning}}

To avoid misspecified model assumptions, it is necessary to do this
automatic variable/variable set identification procedure using data
adaptive machine learning methods. In addition to using a prespecified
non-parametric function to identify these individual mixture, effect
modifier, and interacting variables, it is necessary to establish a
prespecified target parameter that is applied to variable sets when
identified. Of course, both procedures cannot be done on the full data
without resulting in over-fitting the target parameter, therefore it is
necessary to use cross-validation procedures where given a training set
of the data is used to:

\begin{itemize}
\tightlist
\item
  Identify variable sets of interest using flexible machine learning
\item
  Using these variables fit estimators for the relevant nuisance
  parameters for our final target parameter of interest
\item
  Get estimates of our nuisance parameters and final target parameter of
  interest using the held-out validation data
\end{itemize}

This procedure is done in each fold of the CV procedure. Therefore, an
estimate for each fold specific validation data is given. In order to
optimize the optimum bias-variance trade-off for our causal parameter of
interest we use targeted minimum loss based estimation (TMLE).

\hypertarget{method-overview}{%
\subsubsection{Method Overview}\label{method-overview}}

This vignette provides explanation for the package \texttt{IsoXshift}
which first builds a discrete Super Learner with a library of algorithms
that are basis function selectors Using the best estimator from this
algorithm (algorithm with the lowers cross-validated MSE) do ANOVA style
analysis on the appropriate set of basis functions to determine which
variable(s) contribute most to model fit through the basis functions.
Depending on which sets of variables are data-adaptively determined,
apply a target parameter of interest to these variable sets. The target
parameters are explained in more details below.

Firts, let's load the packages we'll use and set a seed for simulation:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(data.table)}
\FunctionTok{library}\NormalTok{(sl3)}
\FunctionTok{library}\NormalTok{(SuperLearner)}
\FunctionTok{library}\NormalTok{(devtools)}
\FunctionTok{library}\NormalTok{(kableExtra)}
\FunctionTok{load\_all}\NormalTok{()}
\FunctionTok{library}\NormalTok{(IsoXshift)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{429151}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{data-and-notation}{%
\subsection{Data and Notation}\label{data-and-notation}}

\hypertarget{our-data-generating-system}{%
\subsubsection{Our Data-Generating
System}\label{our-data-generating-system}}

Consider \(n\) observed units \(O_1, \ldots, O_n\), where each random
variable \(O =(V \subset W, A, Y)\) corresponds to a single
observational unit. Let \(W\) denote baseline covariates (e.g., age,
sex, education level) that are not considered by the analyst to be
effect modifiers of any exposure variable, \(V\) denote a subset of
baseline covariates (measured before exposure) that may be potential
effect modifiers of any exposure, \(A\) denote a single exposure
variable or a vector of exposure variables (mixture) of interest (e.g.,
mixed pesticide biomarkers, metals etc. that are measured at the same
time), and \(Y\) an outcome of interest (e.g.,disease status). Though it
need not be the case, let \(A\) be continuous-valued,
i.e.~\(A \in \mathbb{R}\). Let \(O_i \sim \mathcal{P} \in \mathcal{M}\),
where \(\mathcal{M}\) is the nonparametric statistical model defined as
the set of continuous densities on \(O\) with respect to some dominating
measure. To formalize the definition of stochastic interventions and
their corresponding causal effects, we introduce a nonparametric
structural equation model (NPSEM), based on @pearl2000causality, to
define how the system changes under posited interventions on:

\begin{align*}\label{eqn:npsem}
  W &= f_W(U_W) \\ A &= f_A(W, U_A) \\ Y &= f_Y(A, W, U_Y),
\end{align*}

We denote the observed data structure \(O = (W, A, Y)\).

where the set of structural equations provide a mechanistic model by
which the observed data \(O\) is assumed to have been generated.

\hypertarget{assumptions}{%
\subsubsection{Assumptions}\label{assumptions}}

There are several standard assumptions embedded in the NPSEM --
specifically, a temporal ordering that supposes that \(Y\) occurs after
\(A\), which occurs after \(W\); each variable (i.e., \(\{W, A, Y\}\))
is assumed to have been generated from its corresponding deterministic
function (i.e., \(\{f_W, f_A, f_Y\}\)) of the observed variables that
precede it temporally, as well as an exogenous variable, denoted by
\(U\); lastly, each exogenous variable is assumed to contain all
unobserved causes of the corresponding observed variable.

\hypertarget{factorizing-the-likelihood}{%
\subsubsection{Factorizing the
Likelihood}\label{factorizing-the-likelihood}}

The likelihood of the data \(O\) admits a factorization, wherein, for
\(p_0^O\), the density of \(O\) with respect to the product measure, the
density evaluated on a particular observation \(o\) may be a written
\begin{equation}
  p_0^O(o) = q^O_{0,Y}(y \mid A = a, W = w) q^O_{0,A}(a \mid W = w)
  q^O_{0,W}(w)
\end{equation} where \(q_{0, Y}\) is the conditional density of \(Y\)
given \((A, W)\) with respect to some dominating measure, \(q_{0, A}\)
is the conditional density of \(A\) given \(W\) with respect to
dominating measure \(\mu\), and \(q_{0, W}\) is the density of \(W\)
with respect to dominating measure \(\nu\). Further, for ease of
notation, let \(Q(A, W) = \mathbb{E}[Y \mid A, W]\),
\(g(A \mid W) = \mathbb{P}(A \mid W)\), and \(q_W\) the marginal
distribution of \(W\). These components of the likelihood will be
essential in developing an understanding of the manner in which
stochastic treatment regimes perturb a system and how a corresponding
causal effect may be evaluated.

\hypertarget{data-adaptive-target-parameter-effect-modification}{%
\subsubsection{Data-Adaptive Target Parameter: Effect
Modification}\label{data-adaptive-target-parameter-effect-modification}}

For example, in \texttt{IsoXshift} time ordering is used to determine
our causal parameters of interest. That is, the analyst passes a vector
\(V\) of baseline covariates \(W\) to \texttt{IsoXshift} that may be
effect modifiers of any mixture variable in the vector \(A\). Here, the
target causal parameter is effect modification of the shift parameter
because \(V\) is measured before \(A\) in our NPSEM. The effect
modification parameter is a measure of how different the counterfactual
mean of \(Y\) is given a shift in \(A\) stratified on \(V\).

\hypertarget{data-adaptive-target-parameter-interaction}{%
\subsubsection{Data-Adaptive Target Parameter:
Interaction}\label{data-adaptive-target-parameter-interaction}}

Conversly, if sets of variables are found to be important in the
data-adaptive procedure that are only in \(A\) (a set of mixture
components) because these variables are (in most cases) measured
together at one time (such as biomarkers) the shift parameter is instead
the counterfactual mean of \(Y\) given a simultaneous shift in both
variables in \(A\). This is a measure of interaction, rather than effect
modification, using our shift parameter. In the case that an individual
variable in \(V\) or \(A\) is found to have a marginal impact on \(Y\),
then a simple univariate stochastic shift is done on this variable
alone.

\hypertarget{overview-stochastic-shifts}{%
\subsubsection{Overview Stochastic
Shifts}\label{overview-stochastic-shifts}}

Briefly, let \(A\) denote a continuous-valued exposure or set of
exposures, we assume that the distribution of \(A\) conditional on
\(W = w\) has support in the interval \((l(w), u(w))\) -- for
convenience, let this support be \emph{a.e.} That is, the minimum
natural value of treatment \(A\) for an individual with covariates
\(W = w\) is \(l(w)\); similarly, the maximum is \(u(w)\). Then, a
simple stochastic intervention, based on a shift \(\delta\), may be
defined

\begin{equation}\label{eqn:shift}
  d(a, w) =
  \begin{cases}
    a - \delta & \text{if } a > l(w) + \delta \\
    a & \text{if } a \leq l(w) + \delta,
  \end{cases}
\end{equation}

where \(0 \leq \delta \leq u(w)\) is an arbitrary pre-specified value
that defines the degree to which the observed value \(A\) is to be
shifted, where possible. In \texttt{IsoXshift} there are four different
shift parameters depending on what is found in the data-adaptive ANOVA
within the cross-validated fold - these are discussed below.

\hypertarget{target-parameter}{%
\subsection{Target Parameter}\label{target-parameter}}

Our target parameter of interest is the stochastic intervention of a
data-adaptively identified exposure variable or variables. Stochastic
interventions are a relatively simple yet flexible framework for
defining realistic causal effects. In contrast to intervention regimens
such as the average treatment effect (ATE), stochastic interventions can
be applied to nearly any type of exposure variable (binary, ordinal,
continuous). The resulting stochastic shift parameter can be interpreted
in a similar fashion as coefficients in a regression model.

\hypertarget{types-of-shifts}{%
\subsubsection{Types of Shifts}\label{types-of-shifts}}

This \texttt{IsoXshift} package examines the effects attributable to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  shifting the observed value of an individual exposure up or down by
  some scalar \(\delta\).
\item
  shifting the observed value of an individual exposure within strata of
  an effect modifier up or down by some scalar \(\delta\).
\item
  shifting the observed value of a joint exposure up or down by some
  scalar \(\delta\).
\item
  shifting the observed value of a joint exposure up or down by some
  scalar \(\delta\) within strata of an effect modifier.
\end{enumerate}

The \texttt{IsoXshift} package implements algorithms to data-adaptively
identify mixture variables \(A\) and effect modifiers \(V\) that are
associated with an outcome and for these variable sets compute one-step
or targeted minimum loss-based (TML) estimates of the counterfactual
means changes induced by a shifting function \(\delta(A,W)\).

\hypertarget{data-adative-variable-set-selection}{%
\subsection{Data-Adative Variable Set
Selection}\label{data-adative-variable-set-selection}}

IsoXshift first fits an unrestricted Super Learner for
\(E(Y|W) = h_1(W)\), where \(W\) is a set of baseline covariates that
the analyst wants to control for, that is, baseline covariates with no
suspected effect modification with exposures \(A\). Residuals are
calculated, \(Y^*\), and a second Super Learner is fit
\(E(Y^*|A,V) = h_2(A,V)\) Here, for \(h_2\) we only consider models for
the conditional mean of \(Y^*\) given \(A,V\) that are a function of
linear spline terms and their tensor products. Given a fit of such a
model, we can then partition the variance using classic ANOVA
decompositions by testing the null that some set of coefficient values
(the \(\beta_{s,i}\)) are 0 (e.g., all terms that contain a particular
variable, or represent two-way basis functions). We can then aggregate
partial F-statistics for the basis functions up to the variable level
for variable/interaction importance measures. Algorithms used in
\texttt{IsoXshift} return tensor products of arbitrary order and include
\texttt{earth}, \texttt{polySpline} and \texttt{hal9001}.

\hypertarget{basis-functions}{%
\subsubsection{Basis functions}\label{basis-functions}}

Indicator variables can be used to indicate if a variable \(A\) is less
than or equal to a specific value \(a_s\). The same can be done for
\(A_1, A_2\) to determine if both variable are less than or equal to a
specific value. Thus, a function of our outcome is written as:

\[\psi_{\beta} = \beta_0 + \sum_{s\subset \{1,2,...,p\}}\sum_{i=1}^{n} \beta_{s,i} \phi_{s,i},
    \text{ where } \phi_{s,i} = I(\tilde{A}_{i,s} \leq w_s), A \in \mathbb{R}^p\]
and \(s\) denotes indices of subsets of the \(W\) (e.g., both functions
of single variables and two variables).The placement of knot-points for
every potential value is not feasible in most real-world scenarios and
therefore \texttt{IsoXshift} chooses the best estimator from a class of
basis algorithms. This approximates the exhaustive function.

\hypertarget{defining-important-basis-functions}{%
\subsubsection{Defining Important Basis
Functions}\label{defining-important-basis-functions}}

The user can pass into \texttt{IsoXshift} the parameter
\texttt{quantile\_thresh} which designates the F-statistic threshold by
which basis functions are kept. For example, if the user uses
\texttt{quantile\_thresh\ =\ 0.25} and the resulting vector is
\(M_1, M_1V_1, M_1M_3\) then this indicates that the basis functions
that had an F-statistic above the 25th quantile included basis functions
for \(M_1\) alone, basis functions for \(M_1\) and \(V_1\) and basis
functions for \(M_1\) and \(M_3\).

This means that, within this fold we would train estimators for each of
our estimates of interest, that is, the individual stochastic shift of
\(M_1\), how \(V_1\) modifies the shift relationship between \(M_1\) and
\(Y\), and the joint shift of \(M1,M_3\). Now let's more formally define
these paramters.

\hypertarget{target-parameter-interaction}{%
\subsection{Target Parameter:
Interaction}\label{target-parameter-interaction}}

Once \texttt{IsoXshift} has identified interacting variables with the
highest partial F-statistic from the basis functions we need to get
statistical estimation for how these interactions affect our outcome of
interest.

Our target parameter of interest is a semi-parametric definition of an
interaction, defined as: \begin{equation} \label{eq1}
\begin{split}
E(Y|A_{i^*} &+ \delta_1, A_{j^*} + \delta_2, W) - \\ 
E(Y|A_{i^*} &+ \delta_1, A_{j^*}, W) - \\ 
E(Y|A_{j^*} &+ \delta_2,  A_{i^*}, W) + \\
E(Y|A_{i^*}&, A_{j^*}, W) 
\end{split}
\end{equation}

For the bivariate case A \(\in \{A_{i^*}, A_{j^*}\)\}

This joint causal quantity of interest is identified by:
\(E[Y_{g^o}] = E_W [\int_a E(Y|A=a, W)g^o(a|W)]\), \(g^o(a|W)\) is
similar to the propensity score estimator but here we are estimating
joint densities.

Where in the bivariate case:
\[g^o(a_1, a_2|w) = g(a_1 + \delta_1, a_2 + \delta_2|W = w)\]

Which can be estimated as:

\[g^o(a_1, a_2|w) =  g(a_1 + \delta_1|W = w) * g(a_2 + \delta_2|W = w, A = a_1)\].

We can write our interaction target parameter as:

\[E[Y_{g^o_{\delta_1,\delta_2}}] - E[Y_{g^o_{\delta_1}}] - E[Y_{g^o_{\delta_2}}] + E[Y_{g^o_{1,2}}]\]

Where:

\begin{equation} \label{eq2}
\begin{split}
E[Y_{g^o_{\delta_1,\delta_2}}] &= E_W [\int_{a_{1,\delta_1} a_{2, \delta_2}} E(Y|A=a_{1,\delta_1},a_{2,\delta_2} , W)g^o_{\delta_1,\delta_2}(a|W)]\\
E[Y_{g^o_{\delta_1}}] &= E_W [\int_{a_1} E(Y|A=a_{1, \delta_1}, W)g^o_{\delta_1}(a|W)]\\
E[Y_{g^o_{\delta_2}}] &= E_W [\int_{a_2} E(Y|A=a_{2, \delta_2}, W)g^o_{\delta_2}(a|W)]\\
E[Y_{g^o_{1,2}}] &= E_W [\int_{a_1,a_2} E(Y|A=a_1,a_2, W)g^o_{1,2}(a|W)]
\end{split}
\end{equation}

We can think of this interaction target parameter as the expected
outcome given a joint shift in both exposure variables marginalized over
the joint densities of both variables given both delta shifts subtracted
by the expected outcome given both individual shifts marginalized over
each shifted density respectively and adding back in the expected
outcome under no shift marginalized over the joint density under no
shift intervention.

\hypertarget{target-parameter-effect-modification}{%
\subsection{Target Parameter: Effect
Modification}\label{target-parameter-effect-modification}}

Our effect modification follows in the same vein but instead looks at
the difference in expected outcome under a shift intervention in an
exposure between strata of an effect modifying variable. In the
bivariate case where \(V\) is a binary effect modifier this looks like:

\[E[Y_{g^o_{\delta}} | V = 1] - E[Y_{g^o_{\delta}} | V = 0]\]

Where:

\begin{equation} \label{eq3}
\begin{split}
E[Y_{g^o_{a_\delta}}] &= E_W [\int_{a_{\delta}} E(Y|A=a_{\delta}, W)g^o_{\delta}(a | V = 1, W)]\\

E[Y_{g^o_{a_\delta}}] &= E_W [\int_{a_{\delta}} E(Y|A=a_{\delta}, W)g^o_{\delta}(a | V = 0, W)]
\end{split}
\end{equation}

In words, this can be interpreted as the expected outcome given a shift
in \(A\) within strata where \(V = 1\), marginalized over the
conditional density of A under this stratified shift and minus the
expected outcome given a shift in \(A\) in strata \(V = 0\) marginalized
over the conditional density of A under this stratified shift.

\hypertarget{target-parameter-individual-stochastic-shift}{%
\subsection{Target Parameter: Individual Stochastic
Shift}\label{target-parameter-individual-stochastic-shift}}

In the case where a basis function consists of only 1 variable that is
included in \(A\) or \(V\) and individual stochastic shift is done,
which is simply:

\[E[Y_{g^o_{\delta}}]\] Where:

\[E[Y_{g^o_{\delta}}] = E_W [\int_{a} E(Y|A=a_{ \delta}, W)g^o_{\delta}(a|W)]\]

Or simply, the expected outcome given a shift in \(A\) by \(\delta\)
marginalized over the conditional distribution of \(a\) given such a
shift.

\hypertarget{estimating-the-causal-effect-for-each-stochastic-intervention}{%
\subsection{Estimating the Causal Effect for each Stochastic
Intervention}\label{estimating-the-causal-effect-for-each-stochastic-intervention}}

With identification assumptions satisfied, the efficient influence
function (EIF) with respect to the nonparametric model \(\mathcal{M}\)
can be estimated for each of our parameters of interested based on work
from @diaz2012population and @diaz2018stochastic. The EIF for a
stochastic intervention is:

\begin{equation}
  D(P_0)(x) = H(a, w)({y - \overline{Q}(a, w)}) +
  \overline{Q}(d(a, w), w) - \Psi(P_0),
 
\end{equation} where the auxiliary covariate \(H(a,w)\) may be expressed
\begin{equation}
  H(a,w) = \mathbb{I}(a + \delta < u(w)) \frac{g_0(a - \delta \mid w)}
  {g_0(a \mid w)} + \mathbb{I}(a + \delta \geq u(w)),
  
\end{equation} which may be reduced to \begin{equation}
  H(a,w) = \frac{g_0(a - \delta \mid w)}{g_0(a \mid w)} + 1
 
\end{equation} in the case that the treatment is within the limits that
arise from conditioning on \(W\), i.e., for
\(A_i \in (u(w) - \delta, u(w))\).

This efficient influence function was originally derived to allow for
the construction of a semiparametric-efficient estimators for a
stochastic shift in the univariate setting. The argeted maximum
likelihood (TML) estimator to solve this EIF was formulated in
@diaz2018stochastic by using the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Construct initial density estimators \(g_n\) of \(g_0(A, W)\) and
  \(Q_n\) of \(\overline{Q}_0(A, W)\), using data-adaptive regression
  techniques.
\item
  Predict the density distribution of \(d(A,W)\) under the shift
  \(\delta\)
\item
  For each observation \(i\), compute an estimate \(H_n(a_i, w_i)\) of
  the auxiliary covariate \(H(a_i,w_i)\). This is the ratio of
  conditional densities, the shifted density in the numerator and
  natural density in the denominator.
\item
  Estimate the parameter \(\epsilon\) in the logistic regression model
  \[ \text{logit}\overline{Q}_{\epsilon, n}(a, w) =
  \text{logit}\overline{Q}_n(a, w) + \epsilon H_n(a, w),\] or an
  alternative regression model incorporating weights.
\item
  Compute TML estimator \(\Psi_n\) of the target parameter, defining
  update \(\overline{Q}_n^{\star}\) of the initial estimate
  \(\overline{Q}_{n, \epsilon_n}\): \begin{equation}
    \Psi_n = \Psi(P_n^{\star}) = \frac{1}{n} \sum_{i = 1}^n
    \overline{Q}_n^{\star}(d(A_i, W_i), W_i).
  \end{equation}
\end{enumerate}

\hypertarget{eif-and-tmle-applied-to-our-target-parameters}{%
\subsubsection{EIF and TMLE Applied to our Target
Parameters}\label{eif-and-tmle-applied-to-our-target-parameters}}

The same EIF and TMLE steps to solve the EIF used in the univariate
stochastic shift of a treatment can be used for all our target
parameters of interest. That is, in the case that only one variable in
\(A,V\) was found data-adaptively in basis functions, then the targeting
step is exactly the same as in the original stochastic intervention
work. In the case of effect modification, the same procedure is done
however the conditional densities used in construction of the clever
covariate include the counterfactuals made on the effect modifier.
Construction of initial density estimators for \(g_n\) of
\(g_0(A, V=v, W)\) and \(Q_n\) of \(\overline{Q}_0(A, V=v, W)\), using
data-adaptive regression techniques. As can be seen, once these
densities are estimated, the same TMLE steps can be done to update the
original estimates. Likewise, for our interaction parameter the same
procedure holds but now for joint densitieis. We construct initial
density estimators \(g_n\) of \(g_0(A_1, A_2, W)\) and \(Q_n\) of
\(\overline{Q}_0(A_1, A_2, W)\), using data-adaptive regression
techniques. Then our clever covariate becomes a ratio of the predicted
joint likelihood of a joint shift over the unperturbed joint likelihood.
The subsequent steps then all remain the same.

\hypertarget{data-adaptive-esimation}{%
\subsection{Data-Adaptive Esimation}\label{data-adaptive-esimation}}

The above theory works for stochastic shift situations where the \(A\)
is known \textit{a priori}. This is not the case for mixtures where we
are interested in first identifying which variables are ``important''
given the fit of the best estimator and estimating the stochastic shift
for these variable sets. Because the data is being used to both identify
a target parameter and make estimates given this parameter, sample
splitting (cross-validation) is used to avoid bias from over fitting,
but still uses the entire data set to estimate a data-adaptive
parameter. V-fold cross-validation involves: (i) \({1,..., n}\),
observations, is divided into \(V\) equal size subgroups, (ii) for each
\(v\), an estimation-sample, notationally \(P_{n,v}\) , is defined by
the v-th subgroup of size n/V, while the parameter-generating sample,
\(P_{n,v^c}\), is its complement. More concretely, for split \(v\) the
empirical distribution, \(P_{n,v^c}\), is used to define the exposure(s)
\(A\) given our \(\textit{a priori}\) basis function algorithm which
outputs the target variables based on our ANOVA test of the basis
functions used. The observations not in \(P_{n,v^c}\), namely the
empirical distribution for \(P_{n,v}\) then is used to generate the
parameter of interest. That is, if we had 1000 observations and 4 folds,
our estimation sample \(P_{n,v}\) would be of size 250. For each fold,
the respective 750 observations would be used to train our estimators
\(g_n\) and \(Q_n\) and given these estimators we then get predictions
given the respective 250 observations. The predicted density, \(g_n\)
and outcome \(Q_n\) are then used to construct our clever covariate for
the shift intervention, \(H_n(a_i, w_i)\) for the TMLE update within the
fold. Therefore, TMLE updated estimates are given for each fold using
the respective estimation sample data \(P_{n,v}\).

\hypertarget{application}{%
\subsection{Application:}\label{application}}

Below we show implementation of \texttt{IsoXshift} using simulated data.
Because estimates are given for each data-adaptively identified
parameter for each fold, after the demonstration we also show our pooled
estimate approach for parameters that are found across all the folds.

Simulate some toy data that has both interaction and effect
modification.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_info }\OtherTok{\textless{}{-}}\NormalTok{ IsoXshift}\SpecialCharTok{::}\FunctionTok{simulate\_data}\NormalTok{()}
\NormalTok{data }\OtherTok{\textless{}{-}}\NormalTok{ data\_info}\SpecialCharTok{$}\NormalTok{data}
\end{Highlighting}
\end{Shaded}

Partition the data into nodes that will be passed to \texttt{IsoXshift}

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{W }\OtherTok{\textless{}{-}}\NormalTok{ data[,}\FunctionTok{c}\NormalTok{(}\StringTok{"W2"}\NormalTok{, }\StringTok{"W3"}\NormalTok{)]}
\NormalTok{A }\OtherTok{\textless{}{-}}\NormalTok{ data[,}\FunctionTok{c}\NormalTok{(}\StringTok{"M1"}\NormalTok{, }\StringTok{"M2"}\NormalTok{, }\StringTok{"M3"}\NormalTok{)]}
\NormalTok{V }\OtherTok{\textless{}{-}}\NormalTok{ data[,}\FunctionTok{c}\NormalTok{(}\StringTok{"W1"}\NormalTok{)]}
\NormalTok{Y }\OtherTok{\textless{}{-}}\NormalTok{ data[,}\StringTok{"Y"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

Setup the Super Learner libraries that will be used for each of our
nuisance parameters

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# this library is used for E(Y|W) to calculate Y*, or the remaining variance in Y after removing variance due to W. }
\NormalTok{SL.library }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}SL.randomForest\textquotesingle{}}\NormalTok{,}
               \StringTok{"SL.glm"}\NormalTok{,}
               \StringTok{"SL.mean"}\NormalTok{)}

\CommentTok{\# this is our density estimator for g\_n}
\NormalTok{sl\_density\_lrnr }\OtherTok{\textless{}{-}} \FunctionTok{make\_density\_superlearner}\NormalTok{()}


\CommentTok{\# this is our estimator for Q1\_n {-} the Super Learner made of basis function estimators, E(Y*|A,V)}
\NormalTok{full\_lrn\_ridge }\OtherTok{\textless{}{-}}\NormalTok{ Lrnr\_glmnet}\SpecialCharTok{$}\FunctionTok{new}\NormalTok{(}\AttributeTok{alpha =} \DecValTok{0}\NormalTok{)}
\NormalTok{full\_lrn\_lasso }\OtherTok{\textless{}{-}}\NormalTok{ Lrnr\_glmnet}\SpecialCharTok{$}\FunctionTok{new}\NormalTok{(}\AttributeTok{alpha =} \DecValTok{1}\NormalTok{)}
\NormalTok{full\_lrn\_earth\_1 }\OtherTok{\textless{}{-}}\NormalTok{ Lrnr\_earth}\SpecialCharTok{$}\FunctionTok{new}\NormalTok{(}\AttributeTok{linpreds =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{degree =} \DecValTok{1}\NormalTok{)}
\NormalTok{full\_lrn\_earth\_2 }\OtherTok{\textless{}{-}}\NormalTok{ Lrnr\_earth}\SpecialCharTok{$}\FunctionTok{new}\NormalTok{(}\AttributeTok{linpreds =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{degree =} \DecValTok{2}\NormalTok{)}
\NormalTok{full\_lrn\_earth\_3 }\OtherTok{\textless{}{-}}\NormalTok{ Lrnr\_earth}\SpecialCharTok{$}\FunctionTok{new}\NormalTok{(}\AttributeTok{linpreds =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{degree =} \DecValTok{3}\NormalTok{)}
\NormalTok{full\_lrn\_earth\_4 }\OtherTok{\textless{}{-}}\NormalTok{ Lrnr\_earth}\SpecialCharTok{$}\FunctionTok{new}\NormalTok{(}\AttributeTok{linpreds =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{degree =} \DecValTok{4}\NormalTok{)}


\NormalTok{full\_lrn\_poly\_3 }\OtherTok{\textless{}{-}}\NormalTok{ Lrnr\_polspline}\SpecialCharTok{$}\FunctionTok{new}\NormalTok{(}\AttributeTok{knots =} \DecValTok{3}\NormalTok{)}
\NormalTok{full\_lrn\_poly\_4 }\OtherTok{\textless{}{-}}\NormalTok{ Lrnr\_polspline}\SpecialCharTok{$}\FunctionTok{new}\NormalTok{(}\AttributeTok{knots =} \DecValTok{4}\NormalTok{)}
\NormalTok{full\_lrn\_poly\_5 }\OtherTok{\textless{}{-}}\NormalTok{ Lrnr\_polspline}\SpecialCharTok{$}\FunctionTok{new}\NormalTok{(}\AttributeTok{knots =} \DecValTok{5}\NormalTok{)}
\NormalTok{full\_lrn\_poly\_6 }\OtherTok{\textless{}{-}}\NormalTok{ Lrnr\_polspline}\SpecialCharTok{$}\FunctionTok{new}\NormalTok{(}\AttributeTok{knots =} \DecValTok{6}\NormalTok{)}


\NormalTok{full\_lrn\_glm }\OtherTok{\textless{}{-}}\NormalTok{ Lrnr\_glm}\SpecialCharTok{$}\FunctionTok{new}\NormalTok{()}
\NormalTok{full\_lrn\_mean }\OtherTok{\textless{}{-}}\NormalTok{ Lrnr\_mean}\SpecialCharTok{$}\FunctionTok{new}\NormalTok{()}

\NormalTok{learners }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}
\NormalTok{  full\_lrn\_earth\_1,}
\NormalTok{  full\_lrn\_earth\_2,}
\NormalTok{  full\_lrn\_earth\_3,}
\NormalTok{  full\_lrn\_earth\_4,}
\NormalTok{  full\_lrn\_poly\_3,}
\NormalTok{  full\_lrn\_poly\_4,}
\NormalTok{  full\_lrn\_poly\_5,}
\NormalTok{  full\_lrn\_poly\_6}

\NormalTok{)}

\FunctionTok{names}\NormalTok{(learners) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}
  \StringTok{"full earth 1"}\NormalTok{,}
  \StringTok{"full earth 2"}\NormalTok{,}
  \StringTok{"full earth 3"}\NormalTok{,}
  \StringTok{"full earth 4"}\NormalTok{,}
  \StringTok{"full poly 3"}\NormalTok{,}
  \StringTok{"full poly 4"}\NormalTok{,}
  \StringTok{"full poly 5"}\NormalTok{,}
  \StringTok{"full poly 6"}
\NormalTok{)}

\NormalTok{Q1\_stack }\OtherTok{\textless{}{-}} \FunctionTok{make\_learner}\NormalTok{(Stack, learners)}

\CommentTok{\# this is our estimator for Q2\_n, the outcome estimation E(Y|A,W)}
\NormalTok{mean\_lrnr }\OtherTok{\textless{}{-}}\NormalTok{ Lrnr\_mean}\SpecialCharTok{$}\FunctionTok{new}\NormalTok{()}
\NormalTok{fglm\_lrnr }\OtherTok{\textless{}{-}}\NormalTok{ Lrnr\_glm\_fast}\SpecialCharTok{$}\FunctionTok{new}\NormalTok{()}
\NormalTok{rf\_lrnr }\OtherTok{\textless{}{-}}\NormalTok{ Lrnr\_ranger}\SpecialCharTok{$}\FunctionTok{new}\NormalTok{()}
\NormalTok{lasso\_learner }\OtherTok{\textless{}{-}}\NormalTok{ Lrnr\_glmnet}\SpecialCharTok{$}\FunctionTok{new}\NormalTok{(}\AttributeTok{alpha =} \DecValTok{1}\NormalTok{)}
\NormalTok{ridge\_learner }\OtherTok{\textless{}{-}}\NormalTok{ Lrnr\_glmnet}\SpecialCharTok{$}\FunctionTok{new}\NormalTok{(}\AttributeTok{alpha =} \DecValTok{0}\NormalTok{)}
\NormalTok{lrn\_polspline }\OtherTok{\textless{}{-}}\NormalTok{ Lrnr\_polspline}\SpecialCharTok{$}\FunctionTok{new}\NormalTok{()}
\NormalTok{lrn\_ranger100 }\OtherTok{\textless{}{-}} \FunctionTok{make\_learner}\NormalTok{(Lrnr\_ranger, }\AttributeTok{num.trees =} \DecValTok{100}\NormalTok{)}
\NormalTok{hal\_lrnr }\OtherTok{\textless{}{-}}\NormalTok{ Lrnr\_hal9001}\SpecialCharTok{$}\FunctionTok{new}\NormalTok{(}\AttributeTok{max\_degree =} \DecValTok{3}\NormalTok{, }\AttributeTok{n\_folds =} \DecValTok{3}\NormalTok{)}

\NormalTok{Q2\_stack }\OtherTok{\textless{}{-}} \FunctionTok{make\_learner}\NormalTok{(}
\NormalTok{  Stack, mean\_lrnr, fglm\_lrnr, rf\_lrnr, lasso\_learner, ridge\_learner, lrn\_polspline, lrn\_ranger100}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Run the \texttt{IsoXshift} main function passing in the data, number of
folds, limit of detection censoring, delta to shift by, type of
estimator and fluctuation, stacks of SL estimators for each nuisance
parameter, quantile to threshold basis functions and whether to parallel
process.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_results }\OtherTok{\textless{}{-}}\NormalTok{ IsoXshift}\SpecialCharTok{::}\FunctionTok{IsoXshift}\NormalTok{(}\AttributeTok{W =}\NormalTok{ W,}
                         \AttributeTok{V =}\NormalTok{ V,}
                         \AttributeTok{A =}\NormalTok{ A,}
                         \AttributeTok{Y =}\NormalTok{ Y,}
                         \AttributeTok{n\_folds =} \DecValTok{3}\NormalTok{,}
                         \AttributeTok{LOD\_cens =} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FunctionTok{dim}\NormalTok{(A)[}\DecValTok{1}\NormalTok{]),}
                         \AttributeTok{delta =} \DecValTok{1}\NormalTok{,}
                         \AttributeTok{estimator =} \StringTok{"tmle"}\NormalTok{,}
                         \AttributeTok{fluctuation =} \StringTok{"standard"}\NormalTok{,}
                         \AttributeTok{max\_iter =} \DecValTok{10}\NormalTok{,}
                         \AttributeTok{LOD\_fit\_args =} \FunctionTok{list}\NormalTok{(}
                         \AttributeTok{fit\_type =} \FunctionTok{c}\NormalTok{(}\StringTok{"glm"}\NormalTok{),}
                         \AttributeTok{sl\_learners =} \ConstantTok{NULL}
\NormalTok{                         ),}
                         \AttributeTok{sl\_density\_lrnr =}\NormalTok{ sl\_density\_lrnr,}
                         \AttributeTok{SL.library =}\NormalTok{ SL.library,}
                         \AttributeTok{Q1\_stack =}\NormalTok{ Q1\_stack,}
                         \AttributeTok{Q2\_stack =}\NormalTok{ Q2\_stack,}
                         \AttributeTok{family =} \StringTok{"gaussian"}\NormalTok{,}
                         \AttributeTok{quantile\_thresh =} \FloatTok{0.25}\NormalTok{,}
                         \AttributeTok{parallel =} \ConstantTok{TRUE}\NormalTok{) }

\NormalTok{indiv\_shift\_results }\OtherTok{\textless{}{-}}\NormalTok{ sim\_results}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Indiv Shift Results}\StringTok{\textasciigrave{}}
\NormalTok{em\_results }\OtherTok{\textless{}{-}}\NormalTok{ sim\_results}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Effect Mod Results}\StringTok{\textasciigrave{}}
\NormalTok{joint\_shift\_results }\OtherTok{\textless{}{-}}\NormalTok{ sim\_results}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Joint Shift Results}\StringTok{\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

\hypertarget{pooling-estimates-found-across-the-folds}{%
\subsection{Pooling Estimates Found Across the
Folds}\label{pooling-estimates-found-across-the-folds}}

Because \texttt{IsoXshift} gets the TMLE updated shift parameter for
each condition (individual, effect modification or joint), for each
fold, when many folds are used the resulting tables can be very large.
For estimates that are found across all folds (are consistently found in
the data adaptive procedure) we can pool the effects across the folds
similar to a meta-analysis. We should therefore give effect sizes with a
higher precision (i.e.~a smaller standard error derived from the
efficient influence function (EIF)) a greater weight. If we want to
calculate the pooled effect size under the fixed-effect model, we
therefore simply use a weighted average of all studies. To calculate the
weight \(w_k\) for each fold \(k\), we can use the standard error, which
we square to obtain the variance \(s^2_k\) of each effect size. Since a
lower variance indicates higher precision, the inverse of the variance
is used to determine the weight of each fold estimate.

\[w_k = \frac{1}{s^2_k}\]

Once we know the weights, we can calculate the weighted average, our
estimate of the true pooled effect \(\hat{\psi}\) We only have to
multiply each fold's effect size \(\hat{\theta}_k\) with its
corresponding weight \(w_k\), sum the results across all studies \(K\)
in our pooled analysis, and then divide by the sum of all the individual
weights.

\[\hat{\psi} = \frac{\sum_{k = 1}^K \hat{\Psi}_k w_k}{\sum_{k = 1}^K w_k}\]
Because we use the inverse of the variance, we can call this parameter
the inverse-variance pooled-fold-analysis.

The variance of the combined effect across the folds is defined as the
reciprocal of the sum of the weights, or
\(v. = \frac{1}{\sum_{i=1}^K w_i}\) and likewise the standard error is
\(\sqrt{v.}\). Pooled confidence intervals and p-values are calculated
in the normal way based on the calculated \(\hat{\psi}\) and \(v.\).
Below we show the results tables output by the \texttt{IsoXshift} for
each parameter for each fold then we show output from the
\texttt{compute\_meta\_results} function which calculates the pooled
esimtates and creates plots.

\hypertarget{results-individual-stochastic-shifts}{%
\subsection{Results Individual Stochastic
Shifts}\label{results-individual-stochastic-shifts}}

The expected outcome for individual stochastic shifting of
data-adaptively identified exposures in a mixture is given in
\texttt{Indiv\ Shift\ Results} from the \texttt{IsoXshift} fit object.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{indiv\_shift\_results }\OtherTok{\textless{}{-}}\NormalTok{ sim\_results}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Indiv Shift Results}\StringTok{\textasciigrave{}}
\FunctionTok{setorder}\NormalTok{(indiv\_shift\_results, Condition)}
\NormalTok{indiv\_shift\_results}

\NormalTok{indiv\_shift\_results }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{kbl}\NormalTok{(}\AttributeTok{caption =} \StringTok{"Individual Stochastic Shift Results"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{kable\_classic}\NormalTok{(}\AttributeTok{full\_width =}\NormalTok{ F, }\AttributeTok{html\_font =} \StringTok{"Cambria"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The expected outcome for individual stochastic shifting of
data-adaptively identified exposure in a mixture through strata of a
data-adaptively identified effect modifier is given
in\texttt{Effect\ Mod\ Results} from the \texttt{IsoXshift} fit object.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{em\_results }\OtherTok{\textless{}{-}}\NormalTok{ sim\_results}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Effect Mod Results}\StringTok{\textasciigrave{}}
\FunctionTok{setorder}\NormalTok{(em\_results, Fold)}
\NormalTok{em\_results }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{kbl}\NormalTok{(}\AttributeTok{caption =} \StringTok{"Effect Modification Stochastic Shift Results"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{kable\_classic}\NormalTok{(}\AttributeTok{full\_width =}\NormalTok{ F, }\AttributeTok{html\_font =} \StringTok{"Cambria"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The expected outcome for joint stochastic shifting of data-adaptively
identified set of exposures in a mixture is given
in\texttt{Effect\ Mod\ Results} from the \texttt{IsoXshift} fit object.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{joint\_shift\_results }\OtherTok{\textless{}{-}}\NormalTok{ sim\_results}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Joint Shift Results}\StringTok{\textasciigrave{}}
\FunctionTok{setorder}\NormalTok{(joint\_shift\_results, Fold)}

\NormalTok{joint\_shift\_results }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{kbl}\NormalTok{(}\AttributeTok{caption =} \StringTok{"Joint Stochastic Shift Results"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{kable\_classic}\NormalTok{(}\AttributeTok{full\_width =}\NormalTok{ F, }\AttributeTok{html\_font =} \StringTok{"Cambria"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{indiv\_plots }\OtherTok{\textless{}{-}} \FunctionTok{compute\_meta\_results}\NormalTok{(indiv\_shift\_results, }\AttributeTok{parameter =} \StringTok{"Indiv Results"}\NormalTok{)}
\NormalTok{indiv\_plots}\SpecialCharTok{$}\NormalTok{M1}
\NormalTok{indiv\_plots}\SpecialCharTok{$}\NormalTok{V}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{em\_plots }\OtherTok{\textless{}{-}} \FunctionTok{compute\_meta\_results}\NormalTok{(em\_results, }\AttributeTok{parameter =} \StringTok{"Effect Mod"}\NormalTok{)}
\FunctionTok{names}\NormalTok{(em\_plots)}
\NormalTok{em\_plots}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Effect Mod}\StringTok{\textasciigrave{}}
\NormalTok{em\_plots}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{M1 V0}\StringTok{\textasciigrave{}}
\NormalTok{em\_plots}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{M1 V1}\StringTok{\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{joint\_plots }\OtherTok{\textless{}{-}} \FunctionTok{compute\_meta\_results}\NormalTok{(joint\_shift\_results, }\AttributeTok{parameter =} \StringTok{"Joint Shift"}\NormalTok{)}
\FunctionTok{names}\NormalTok{(joint\_plots)}
\NormalTok{joint\_plots}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Psi{-}M1M2}\StringTok{\textasciigrave{}}
\NormalTok{joint\_plots}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Psi{-}M1M3}\StringTok{\textasciigrave{}}
\end{Highlighting}
\end{Shaded}


\end{document}
